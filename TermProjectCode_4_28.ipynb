{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8ec7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.0.3\n",
      "NumPy version: 1.24.3\n",
      "Scikit-learn version: 1.3.0\n",
      "Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following lines to install required packages if they are not already installed\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Pandas for handling CSV files\n",
    "import pandas as pd\n",
    "\n",
    "# For printing confusion matrix/coefficients\n",
    "import numpy as np\n",
    "\n",
    "# For printing results to CSV\n",
    "import csv\n",
    "\n",
    "# Import scikit learn and necessary modules\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Verify version numbers\n",
    "# print(f\"Pandas version: {pd.__version__}\")\n",
    "# print(f\"NumPy version: {np.__version__}\")\n",
    "# print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# import sys\n",
    "# print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85617843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data into pandas\n",
    "def loaddata(filename, column):\n",
    "    full = pd.read_csv(filename)\n",
    "    full[column] = full[column].fillna(\"\") #pandas method to fill NaN values\n",
    "    data = list(full[column])\n",
    "    return data\n",
    "\n",
    "# Load training and test data\n",
    "train_txt = loaddata(\"train.csv\", \"TEXT\")\n",
    "train_labels = loaddata(\"train.csv\", \"LABEL\")\n",
    "test_ids = loaddata(\"test.csv\", \"ID\")\n",
    "test_txt = loaddata(\"test.csv\", \"TEXT\")\n",
    "\n",
    "# # count labels per class\n",
    "# from collections import Counter\n",
    "# label_counts = Counter(train_labels)\n",
    "# print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001be2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer for getting features\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "\n",
    "    ngram_range=(1,5),\n",
    "    binary=True,\n",
    "    max_df=0.7,\n",
    "    min_df=3,\n",
    "    norm='l2',\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "#     token_pattern=r'(?u)\\b(?!\\bbr\\b)\\w\\w+\\b' #this removes br left over from html tags, but seems to decrease F1 score\n",
    "\n",
    ")\n",
    "\n",
    "# Transform text data to TF-IDF features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_txt)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_txt)\n",
    "\n",
    "# Find K Best Features (finds most statistically relevant features with respect to labels)\n",
    "k_best = SelectKBest(score_func=chi2, k=215000)\n",
    "X_train_kbest = k_best.fit_transform(X_train_tfidf, train_labels)\n",
    "X_test_kbest = k_best.transform(X_test_tfidf)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "# Split data into training and development sets\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train_kbest, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# For printing misclassifications later using raw text (same split/random state so should line up)\n",
    "train_txt_train, train_txt_dev = train_test_split(train_txt, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression model on training data\n",
    "model = LogisticRegression(\n",
    "\n",
    "    solver='saga',\n",
    "    penalty='l2',\n",
    "    C=5,\n",
    "    max_iter = 2000,\n",
    "    class_weight = 'balanced' # more 0 labels, so makes sense to balance classes\n",
    ")  \n",
    "\n",
    "# Train the logistic regression model using the selected features and corresponding labels\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluate model using F1 score and accuracy\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_dev_pred = model.predict(X_dev)\n",
    "\n",
    "# Get F1 Scores\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "dev_f1 = f1_score(y_dev, y_dev_pred, average='weighted')\n",
    "print(\"Development F1 Score:\", dev_f1)\n",
    "print(\"Training F1 Score:\", train_f1)\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = model.predict(X_test_kbest)\n",
    "decoded_test_predictions = label_encoder.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e0fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save predictions to a CSV file\n",
    "# decoded_test_predictions_list = decoded_test_predictions.tolist()  # Convert numpy array to list\n",
    "# with open('results.csv', 'w', newline='') as csv_file:\n",
    "#     csv_writer = csv.writer(csv_file)\n",
    "#     csv_writer.writerow(['ID', 'LABEL'])\n",
    "#     for idnum, prediction in zip(test_ids, decoded_test_predictions_list):\n",
    "#         csv_writer.writerow([idnum, prediction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f19084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print misclassifications\n",
    "def print_misclassified(texts, labels, predictions, set_name):\n",
    "    decoded_labels = label_encoder.inverse_transform(labels)\n",
    "    decoded_predictions = label_encoder.inverse_transform(predictions)\n",
    "    print(f\"\\nMisclassified examples in {set_name} set:\")\n",
    "    for text, true_label, pred_label in zip(texts, decoded_labels, decoded_predictions):\n",
    "        if true_label != pred_label:\n",
    "            print(f\"Text: {text} \\nTrue label: {true_label}, \\nPredicted: {pred_label}\",\"\\n\")\n",
    "            print()\n",
    "\n",
    "# print_misclassified(train_txt_train, y_train, y_train_pred, \"training\")\n",
    "print_misclassified(train_txt_dev, y_dev, y_dev_pred, \"development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print features before SelectKBest\n",
    "print(\"Total number of features before SelectKBest:\", X_train_tfidf.shape[1], \"\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_dev, y_dev_pred)\n",
    "\n",
    "# Get class labels\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Print confusion matrix with row and column labels\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=[f\"Actual {label}\" for label in class_labels], \n",
    "                     columns=[f\"Predicted {label}\" for label in class_labels])\n",
    "\n",
    "print(\"Confusion Matrix for Development Set:\")\n",
    "print(cm_df, \"\\n\")\n",
    "\n",
    "# Get feature names from TF-IDF Vectorizer\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get coefficients from the Logistic Regression model\n",
    "coefficients = model.coef_\n",
    "\n",
    "# Find top features for classes\n",
    "top_n = 10  # Number of top features to show\n",
    "print(\"Top Features:\")\n",
    "for class_index in range(coefficients.shape[0]): # (3 classes)\n",
    "    \n",
    "    # Sort coefficients for class and get the top n\n",
    "    top_features_indices = np.argsort(coefficients[class_index])[-top_n:]\n",
    "    \n",
    "    # Get feature names\n",
    "    top_features = feature_names[k_best.get_support()][top_features_indices]\n",
    "    \n",
    "    # Print top features for class\n",
    "    print(f\"Class {label_encoder.inverse_transform([class_index])[0]}:\")\n",
    "    for feature in top_features:\n",
    "        print(feature)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19460a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de43ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV for Parameter Testing\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define pipeline with some of my existing params\n",
    "# pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(lowercase=True, ngram_range=(1,3), binary=True, max_df=0.7, min_df=3, norm='l2', sublinear_tf=True)),\n",
    "#     ('kbest', SelectKBest(score_func=chi2, k=215000)),\n",
    "#     ('logreg', LogisticRegression(class_weight='balanced'))\n",
    "# ])\n",
    "\n",
    "# # Create parameter grid\n",
    "# param_grid = {\n",
    "#     'tfidf__norm': ['l1', 'l2'],\n",
    "# #     'kbest__k': [200000, 215000, 230000, 250000],\n",
    "#     'logreg__solver': ['sag', 'saga', 'lbfgs'],\n",
    "#     'logreg__C': [3,5,7],\n",
    "#     'logreg__max_iter': [1000, 2000, 3000]\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, scoring='f1_weighted', cv=3, verbose=1)\n",
    "\n",
    "# # Fit GridSearchCV on training data\n",
    "# grid_search.fit(train_txt, train_labels)\n",
    "\n",
    "# # Print best parameters and best score\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# # Get best model\n",
    "# best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# # Split the data again\n",
    "# X_train, X_dev, y_train, y_dev = train_test_split(train_txt, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Evaluate using the best model from GridSearch\n",
    "# y_dev_pred = best_pipeline.predict(X_dev)\n",
    "# dev_f1 = f1_score(label_encoder.transform(y_dev), label_encoder.transform(y_dev_pred), average='weighted')\n",
    "\n",
    "# print(\"Development F1 Score:\", dev_f1)\n",
    "\n",
    "# # Predict on test data\n",
    "# test_predictions = best_pipeline.predict(test_txt)\n",
    "# decoded_test_predictions = label_encoder.inverse_transform(label_encoder.transform(test_predictions)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3643bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
