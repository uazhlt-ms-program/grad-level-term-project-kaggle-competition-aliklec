{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c526c8",
   "metadata": {},
   "source": [
    "# Logistic Regression Model using TF-IDF and Select K Best for Review Classification\n",
    "\n",
    "Ling 539 Term Project\n",
    "\n",
    "Alison Kleczewski\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d8ec7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas version: 2.0.3\n",
      "NumPy version: 1.24.3\n",
      "Scikit-learn version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following lines to install required packages if they are not already installed\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Pandas for handling CSV files\n",
    "import pandas as pd\n",
    "\n",
    "# For printing confusion matrix/coefficients\n",
    "import numpy as np\n",
    "\n",
    "# For printing results to CSV\n",
    "import csv\n",
    "\n",
    "# Import scikit learn and necessary modules\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Verify version numbers\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85617843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data into pandas\n",
    "def loaddata(filename, column):\n",
    "    full = pd.read_csv(filename)\n",
    "    full[column] = full[column].fillna(\"\") #pandas method to fill NaN values\n",
    "    data = list(full[column])\n",
    "    return data\n",
    "\n",
    "# Load training and test data\n",
    "train_txt = loaddata(\"train.csv\", \"TEXT\")\n",
    "train_labels = loaddata(\"train.csv\", \"LABEL\")\n",
    "test_ids = loaddata(\"test.csv\", \"ID\")\n",
    "test_txt = loaddata(\"test.csv\", \"TEXT\")\n",
    "\n",
    "# # count labels per class to determine balance\n",
    "# from collections import Counter\n",
    "# label_counts = Counter(train_labels)\n",
    "# print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001be2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development F1 Score: 0.9457693372790625\n",
      "Training F1 Score: 0.9942233726158887\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer for getting features\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "\n",
    "    ngram_range=(1,5),\n",
    "    binary=True,\n",
    "    max_df=0.7,\n",
    "    min_df=3,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    #token_pattern=r'(?u)\\b(?!\\bbr\\b)\\w\\w+\\b'  # uncomment to remove br tags\n",
    "\n",
    ")\n",
    "\n",
    "# Transform text data to TF-IDF features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_txt)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_txt)\n",
    "\n",
    "# Find K Best Features (most statistically relevant features with respect to labels)\n",
    "k_best = SelectKBest(score_func=chi2, k=215000)\n",
    "X_train_kbest = k_best.fit_transform(X_train_tfidf, train_labels)\n",
    "X_test_kbest = k_best.transform(X_test_tfidf)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "# Split data into 80/20 training and development sets\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train_kbest, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# For printing misclassifications later using raw text (same split/random state so should line up)\n",
    "train_txt_train, train_txt_dev = train_test_split(train_txt, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configure logistic regression model\n",
    "model = LogisticRegression(\n",
    "    solver='saga',\n",
    "    C=5,\n",
    "    max_iter = 2000,\n",
    "    class_weight = 'balanced' # greater number of 0 labels, so balance the classes\n",
    ")  \n",
    "\n",
    "# Train the logistic regression model using the selected features and corresponding labels\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for training and development\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_dev_pred = model.predict(X_dev)\n",
    "\n",
    "# Get F1 Scores for training and development\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "dev_f1 = f1_score(y_dev, y_dev_pred, average='weighted')\n",
    "print(\"Development F1 Score:\", dev_f1)\n",
    "print(\"Training F1 Score:\", train_f1)\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = model.predict(X_test_kbest)\n",
    "decoded_test_predictions = label_encoder.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1e23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to a CSV file\n",
    "decoded_test_predictions_list = decoded_test_predictions.tolist()\n",
    "with open('results.csv', 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['ID', 'LABEL'])\n",
    "    for idnum, prediction in zip(test_ids, decoded_test_predictions_list):\n",
    "        csv_writer.writerow([idnum, prediction])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012fd91",
   "metadata": {},
   "source": [
    "# Further Model Exploration\n",
    "The cells below are for exploring and testing the model. They can be uncommented as needed to do things like print misclassifications, confusion matrix, top features, and to perform Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56d7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to print misclassifications\n",
    "# def print_misclassified(texts, labels, predictions, set_name):\n",
    "#     decoded_labels = label_encoder.inverse_transform(labels)\n",
    "#     decoded_predictions = label_encoder.inverse_transform(predictions)\n",
    "#     print(f\"\\nMisclassified examples in {set_name} set:\")\n",
    "#     for text, true_label, pred_label in zip(texts, decoded_labels, decoded_predictions):\n",
    "#         if true_label != pred_label:\n",
    "#             print(f\"Text: {text} \\nTrue label: {true_label}, \\nPredicted: {pred_label}\",\"\\n\")\n",
    "\n",
    "# print_misclassified(train_txt_train, y_train, y_train_pred, \"training\")\n",
    "# print_misclassified(train_txt_dev, y_dev, y_dev_pred, \"development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9134ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print number of features before SelectKBest\n",
    "# print(\"Total number of features before SelectKBest:\", X_train_tfidf.shape[1], \"\\n\")\n",
    "\n",
    "# # Create confusion matrix for development set\n",
    "# cm = confusion_matrix(y_dev, y_dev_pred)\n",
    "\n",
    "# # Get class labels\n",
    "# class_labels = label_encoder.classes_\n",
    "\n",
    "# # Print confusion matrix with row and column labels\n",
    "# cm_df = pd.DataFrame(cm, \n",
    "#                      index=[f\"Actual {label}\" for label in class_labels], \n",
    "#                      columns=[f\"Predicted {label}\" for label in class_labels])\n",
    "\n",
    "# print(\"Confusion Matrix for Development Set:\")\n",
    "# print(cm_df, \"\\n\")\n",
    "\n",
    "# # Get feature names from TF-IDF Vectorizer\n",
    "# feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# # Get coefficients from the model\n",
    "# coefficients = model.coef_\n",
    "\n",
    "# # Find top features for classes\n",
    "# top_n = 10  # Number of top features to show\n",
    "# print(\"Top Features:\")\n",
    "# for class_index in range(coefficients.shape[0]): # (3 classes)\n",
    "    \n",
    "#     # Sort coefficients for class and get the top n\n",
    "#     top_features_indices = np.argsort(coefficients[class_index])[-top_n:]\n",
    "    \n",
    "#     # Get feature names\n",
    "#     top_features = feature_names[k_best.get_support()][top_features_indices]\n",
    "    \n",
    "#     # Print top features for class\n",
    "#     print(f\"Class {label_encoder.inverse_transform([class_index])[0]}:\")\n",
    "#     for feature in top_features:\n",
    "#         print(feature)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71de43ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GridSearchCV for Parameter Testing\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define pipeline with some of my existing params\n",
    "# pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(lowercase=True, ngram_range=(1,5), binary=True, norm='l2', sublinear_tf=True)),\n",
    "#     ('kbest', SelectKBest(score_func=chi2, k=215000)),\n",
    "#     ('logreg', LogisticRegression(penalty='l2', C=5, class_weight = 'balanced', solver = 'saga'))\n",
    "# ])\n",
    "\n",
    "# # Create parameter grid\n",
    "# param_grid = {\n",
    "#     'tfidf__norm': ['l1', 'l2'],\n",
    "#     'tfidf__token_pattern': [r'(?u)\\b(?!\\bbr\\b)\\w\\w+\\b','(?u)\\b\\w\\w+\\b'],\n",
    "#     'tfidf__max_df': [0.6,0.7,0.8],\n",
    "#     'tfidf__min_df': [2,3,4,5],\n",
    "#     'kbest__k': [200000, 215000, 230000, 250000],\n",
    "#     'logreg__solver': ['sag', 'saga', 'lbfgs'],\n",
    "#     'logreg__C': [3,5,7],\n",
    "#     'logreg__max_iter': [1000, 2000, 3000]\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, scoring='f1_weighted', cv=3, verbose=1)\n",
    "\n",
    "# # Fit GridSearchCV on training data\n",
    "# grid_search.fit(train_txt, train_labels)\n",
    "\n",
    "# # Print best parameters and best score\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# # Get best model\n",
    "# best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# # Split the data again\n",
    "# X_train, X_dev, y_train, y_dev = train_test_split(train_txt, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Evaluate using the best model from GridSearch\n",
    "# y_dev_pred = best_pipeline.predict(X_dev)\n",
    "# dev_f1 = f1_score(label_encoder.transform(y_dev), label_encoder.transform(y_dev_pred), average='weighted')\n",
    "\n",
    "# print(\"Development F1 Score:\", dev_f1)\n",
    "\n",
    "# # Predict on test data\n",
    "# test_predictions = best_pipeline.predict(test_txt)\n",
    "# decoded_test_predictions = label_encoder.inverse_transform(label_encoder.transform(test_predictions)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b71e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
